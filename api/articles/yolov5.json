{"title":"yolov5","uid":"f0da541dcd6df6287dc7b7bcefa5bded","slug":"yolov5","date":"2023-12-16T12:04:59.000Z","updated":"2024-01-20T10:06:25.609Z","comments":true,"path":"api/articles/yolov5.json","keywords":null,"cover":[],"content":"<p><img src=\"/../images/image-20231216210026711.png\" alt=\"image-20231216210026711\"></p>\n<p><img src=\"/../images/image-20231216210245075.png\" alt=\"image-20231216210245075\"></p>\n<h4 id=\"IOU-—-交并比\"><a href=\"#IOU-—-交并比\" class=\"headerlink\" title=\"IOU —-交并比\"></a>IOU —-交并比</h4><p><img src=\"/../images/image-20231216210441830.png\" alt=\"image-20231216210441830\"></p>\n<p>Lou为1意味着预测边界框和地面真实边界框完全重叠。<br>您可以为LOU设置阈值，以确定对象检测是否有效。<br>假设您将LOU设置为0.5，在这种情况下。<br>·如果LOU≥为0.5，则将目标检测归类为真阳性(TP)。<br>如果LOU&lt;0.5，则为错误检测，并将其归类为假阳性(FP)。<br>当图像中存在地面真实且模型未能检测到目标时，分类。<br>作为假阴性(FN)。<br>真负片(TN)：TN是我们没有预测到物体的图像的每一部分。<br>度量对于目标检测没有用处，因此我们忽略TN。</p>\n<p><img src=\"/../images/image-20231216210636311.png\" alt=\"image-20231216210636311\"></p>\n<h4 id=\"AP-MAP\"><a href=\"#AP-MAP\" class=\"headerlink\" title=\"AP,MAP\"></a>AP,MAP</h4><p><img src=\"/../images/image-20231216210923498.png\" alt=\"image-20231216210923498\"></p>\n<p><img src=\"/../images/image-20231216211711468.png\" alt=\"image-20231216211711468\"></p>\n<p><img src=\"/../images/image-20231216211921852.png\" alt=\"image-20231216211921852\"></p>\n<p><img src=\"/../images/image-20231216204704657.png\" alt=\"image-20231216204704657\">   </p>\n<p><img src=\"/../images/image-20231216204353728.png\" alt=\"image-20231216204353728\"> </p>\n<h4 id=\"网络架构和组件\"><a href=\"#网络架构和组件\" class=\"headerlink\" title=\"网络架构和组件\"></a>网络架构和组件</h4><p>单阶段检测器：</p>\n<p><img src=\"/../images/image-20231216205751067.png\" alt=\"image-20231216205751067\"></p>\n<p>yolov5：（没有划出专门的颈部Neck）</p>\n<p><img src=\"/../images/image-20231216212300844.png\" alt=\"image-20231216212300844\"></p>\n<p>git clone <a href=\"https://github.moeyy.xyz/https://github.com/ultralytics/yolov5.git\">https://github.moeyy.xyz/https://github.com/ultralytics/yolov5.git</a></p>\n<p><img src=\"/../images/image-20231217123703969.png\" alt=\"image-20231217123703969\"></p>\n<ol>\n<li><code>nc: 80</code>：这个参数表示模型分类数量（number of classes），默认为 80，对应着 COCO 数据集。</li>\n<li><code>depth_multiple: 0.33</code>：这个参数表示模型深度相对于基础版本的倍数。在 YOLOv5 中，有 S、M、L 和 X 四个版本，其中 S 为基础版本，即 <code>depth_multiple: 1.0</code>，而 M、L 和 X 版本为在此基础上分别加深了一定的层数。而 <code>depth_multiple: 0.33</code> 表示在 S 版本的基础上，深度缩小了 3 倍，即变成了 <code>depth_multiple: 0.33</code> × 3 &#x3D; 0.99。</li>\n<li><code>width_multiple: 0.50</code>：这个参数表示模型通道宽度相对于基础版本的倍数。与 <code>depth_multiple</code> 类似，S 版本的 <code>width_multiple</code> 是 1.0，而 M、L 和 X 版本则在此基础上分别扩大了一定的倍数。</li>\n<li><code>anchors</code>：这是一个锚点数组，用于定义不同尺度下的 anchor boxes。YOLOv5 中使用了三个不同的尺度，每个尺度使用三个不同的 anchor boxes。这些锚点大小是相对于输入图像的，因此不同尺度下的大小会有所差别。</li>\n<li><code>backbone</code>：这一部分定义了模型的骨干网络（backbone），包括卷积层、批归一化层和激活函数等。YOLOv5 使用了 CSPDarknet53 这个网络作为基础骨干网络，并在此基础上进行改进。具体而言，YOLOv5 增加了空间注意力机制和SPP模块，以增强特征表达能力。</li>\n<li><code>head</code>：这一部分定义了模型的检测头（detection head），包括检测网络和分类网络。YOLOv5 中的检测网络采用了YOLOv3中的FPN结构，并在此基础上加入了PANet模块和SAM模块，以提高检测性能。</li>\n</ol>\n<p>序列数据的不同采样方法（随机采样和顺序分区）会导致隐状态初始化的差异，原因如下：</p>\n<ol>\n<li>随机采样： 在随机采样中，我们从序列数据中随机选择一个序列作为训练样本。这意味着每次训练时，我们都使用不同的序列作为输入。由于每个序列可能具有不同的上下文和语义信息，模型在每次训练时都需要重新适应不同的序列特征。因此，随机采样会导致隐状态的初始化与之前的训练批次存在一定差异。</li>\n<li>顺序分区： 在顺序分区中，我们按顺序依次读取序列数据进行训练。这意味着模型在每个训练批次中都会接收到相邻的序列数据。由于相邻的序列通常具有相似的上下文和语义信息，模型可以利用之前批次的隐藏状态来帮助理解当前批次的序列。因此，顺序分区会导致隐状态的初始化与之前的训练批次存在一定的相关性。</li>\n</ol>\n<p>不同的隐状态初始化差异可能会对模型的训练和预测产生影响。随机采样可以增加数据的多样性，帮助模型更好地适应不同的序列特征。然而，随机采样可能也会引入一些噪声，导致训练过程更加不稳定。顺序分区可以利用相邻序列之间的相关性，帮助模型更好地捕捉到序列的上下文信息。然而，顺序分区可能会限制模型对不同序列特征的适应能力。</p>\n<p>困惑度（perplexity）是自然语言处理中常用的一个评价指标，主要用于衡量语言模型的预测性能。困惑度越低，表示模型的预测能力越好。</p>\n<p>在自然语言处理中，我们通常使用语言模型来计算文本序列的概率。给定一个文本序列 $W&#x3D;w_1,w_2,…,w_n$，其概率可以表示为：</p>\n<p>$$<br>P(W)&#x3D;P(w_1)\\times P(w_2|w_1) \\times … \\times P(w_n|w_1,w_2,…,w_{n-1})<br>$$</p>\n<p>其中，$P(w_i|w_1,w_2,…,w_{i-1})$ 表示在已知前面 $i-1$ 个词的情况下，第 $i$ 个词的概率。语言模型的目标就是学习这种条件概率分布。在模型训练过程中，我们通常会使用最大似然估计法来估计模型参数。</p>\n<p>困惑度是一个数值指标，表示用当前语言模型对一个测试集进行预测时所得到的困惑程度。具体而言，如果测试集包含 $N$ 个词，我们可以计算出每个词的概率 $P(w_i)$，然后将这些概率求倒数并取对数，即：</p>\n<p>$$<br>\\log \\frac{1}{P(w_1)}+\\log \\frac{1}{P(w_2|w_1)}+…+\\log \\frac{1}{P(w_N|w_1,w_2,…,w_{N-1})}<br>$$</p>\n<p>然后，我们可以将上述结果除以测试集中的词数 $N$，得到平均困惑度。具体而言，平均困惑度的计算公式如下：</p>\n<p>$$<br>\\text{Perplexity}&#x3D;exp\\left(-\\frac{1}{N}\\sum_{i&#x3D;1}^{N}\\log P(w_i)\\right)<br>$$</p>\n<p>例如，如果我们有一个包含100个句子的测试集，其中总共包含1000个词，我们可以使用语言模型来预测每个词的概率，并计算出平均困惑度。假设我们的模型预测准确率较高，平均每个词的概率为0.9，则平均困惑度为：</p>\n<p>$$<br>exp\\left(-\\frac{1}{1000}\\sum_{i&#x3D;1}^{1000}\\log 0.9\\right) \\approx 2.15<br>$$</p>\n<p>这表示我们的模型对测试集中的文本序列进行预测时，每个词的平均困惑度为2.15。如果我们使用一个更好的语言模型，其困惑度可能会更低。</p>\n<p>用困惑度来评价模型确保了不同长度的序列具有可比性</p>\n<h4 id=\"路径聚合网络模块\"><a href=\"#路径聚合网络模块\" class=\"headerlink\" title=\"路径聚合网络模块\"></a>路径聚合网络模块</h4><p><img src=\"/../images/image-20231219133825543.png\" alt=\"image-20231219133825543\"></p>\n<h4 id=\"Focus处理模块\"><a href=\"#Focus处理模块\" class=\"headerlink\" title=\"Focus处理模块\"></a>Focus处理模块</h4><p><img src=\"/../images/image-20231219133907407.png\" alt=\"image-20231219133907407\"></p>\n<h4 id=\"空间金字塔池化模块\"><a href=\"#空间金字塔池化模块\" class=\"headerlink\" title=\"空间金字塔池化模块\"></a>空间金字塔池化模块</h4><p><img src=\"/../images/image-20231219134700073.png\" alt=\"image-20231219134700073\"></p>\n<h4 id=\"跨阶段局部网络模块\"><a href=\"#跨阶段局部网络模块\" class=\"headerlink\" title=\"跨阶段局部网络模块\"></a>跨阶段局部网络模块</h4><p><img src=\"/../images/image-20231219134949252.png\" alt=\"image-20231219134949252\"></p>\n<p><img src=\"/../images/image-20231219135635494.png\" alt=\"image-20231219135635494\"></p>\n<p>​\t</p>\n<h1 id=\"IoU、GIoU、DIoU、CIoU损失函数\"><a href=\"#IoU、GIoU、DIoU、CIoU损失函数\" class=\"headerlink\" title=\"     IoU、GIoU、DIoU、CIoU损失函数         \"></a><a href=\"https://www.cnblogs.com/wujianming-110117/p/13019343.html\">     IoU、GIoU、DIoU、CIoU损失函数         </a></h1><p>IoU、GIoU、DIoU、CIoU损失函数</p>\n<h1 id=\"目标检测任务的损失函数由Classificition-Loss和Bounding-Box-Regeression-Loss两部分构成。目标检测任务中近几年来Bounding-Box-Regression-Loss-Function的演进过程，其演进路线是\"><a href=\"#目标检测任务的损失函数由Classificition-Loss和Bounding-Box-Regeression-Loss两部分构成。目标检测任务中近几年来Bounding-Box-Regression-Loss-Function的演进过程，其演进路线是\" class=\"headerlink\" title=\"目标检测任务的损失函数由Classificition Loss和Bounding Box Regeression Loss两部分构成。目标检测任务中近几年来Bounding Box Regression Loss Function的演进过程，其演进路线是\"></a>目标检测任务的损失函数由Classificition Loss和Bounding Box Regeression Loss两部分构成。目标检测任务中近几年来Bounding Box Regression Loss Function的演进过程，其演进路线是</h1><h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><img src=\"/../images/1251718-20200531153858189-1225302034.png\" alt=\"img\"></h1><h1 id=\"一、IOU-Intersection-over-Union\"><a href=\"#一、IOU-Intersection-over-Union\" class=\"headerlink\" title=\"一、IOU(Intersection over Union)\"></a><strong>一、IOU(Intersection over Union)</strong></h1><p><strong>1.</strong> <strong>特性(优点)</strong></p>\n<p>IoU就是我们所说的<strong>交并比</strong>，是目标检测中最常用的指标，在anchor-based的方法。作用不仅用来确定正样本和负样本，还可以用来评价输出框（predict box）和ground-truth的距离。</p>\n<p> <img src=\"/../images/1251718-20200531153951865-814017809.png\" alt=\"img\"></p>\n<p> \\1. 可以说<strong>它可以反映预测检测框与真实检测框的检测效果。</strong></p>\n<p> \\2. 还有一个很好的特性就是<strong>尺度不变性</strong>，也就是对尺度不敏感（scale invariant）， 在regression任务中，判断predict box和gt的距离最直接的指标就是IoU。**(**<strong>满足非负性；同一性；对称性；三角不等性)</strong></p>\n<p><img src=\"/../images/1251718-20200531154027996-533351609.png\" alt=\"img\"></p>\n<p> <strong>2.</strong> <strong>作为损失函数会出现的问题(缺点)</strong></p>\n<p>\\1. 如果两个框没有相交，根据定义，IoU&#x3D;0，不能反映两者的距离大小（重合度）。同时因为loss&#x3D;0，没有梯度回传，无法进行学习训练。</p>\n<p> \\2. IoU无法精确的反映两者的重合度大小。如下图所示，三种情况IoU都相等，但看得出来他们的重合度是不一样的，左边的图回归的效果最好，右边的最差。</p>\n<p> <img src=\"/../images/1251718-20200531154239104-440943615.png\" alt=\"img\"></p>\n<p> <strong>二、GIOU(Generalized Intersection over Union)</strong></p>\n<p><strong>1****、来源</strong></p>\n<p>在CVPR2019中，论文</p>\n<p>《Generalized Intersection over Union: A Metric and A Loss for Bounding Box Regression》<br> https:arxiv.org&#x2F;abs&#x2F;1902.09630</p>\n<p>提出了GIoU的思想。由于IoU是<strong>比值</strong>的概念，对目标物体的scale是不敏感的。然而检测任务中的BBox的回归损失(MSE loss, l1-smooth loss等）优化和IoU优化不是完全等价的，而且 Ln 范数对物体的scale也比较敏感，IoU无法直接优化没有重叠的部分。</p>\n<p> <img src=\"/../images/1251718-20200531154308726-1360939519.png\" alt=\"img\"></p>\n<p> 这篇论文提出可以直接把IoU设为回归的loss。</p>\n<p> <img src=\"/../images/1251718-20200531154354415-2037289028.png\" alt=\"img\"></p>\n<p> 上面公式的意思是：先计算两个框的最小闭包区域面积_ _(通俗理解：<strong>同时包含了预测框和真实框</strong>的最小框的面积)，再计算出IoU，再计算闭包区域中不属于两个框的区域占闭包区域的比重，最后用IoU减去这个比重得到GIoU。</p>\n<p> 附：<a href=\"https://github.com/generalized-iou/g-darknet\">https://github.com/generalized-iou/g-darknet</a></p>\n<p><strong>2****、 特性</strong>[1]</p>\n<p>与IoU相似，GIoU也是一种距离度量，作为损失函数的话， ,满足损失函数的基本要求</p>\n<p>GIoU对scale不敏感</p>\n<p>GIoU是IoU的下界，在两个框无线重合的情况下，IoU&#x3D;GIoU</p>\n<p>IoU取值[0,1]，但GIoU有对称区间，取值范围[-1,1]。在两者重合的时候取最大值1，在两者无交集且无限远的时候取最小值-1，因此GIoU是一个非常好的距离度量指标。</p>\n<p>与IoU只关注重叠区域不同，<strong>GIoU****不仅关注重叠区域，还关注其他的非重合区域</strong>，能更好的反映两者的重合度。</p>\n<p> <img src=\"/../images/1251718-20200531154522456-385805448.png\" alt=\"img\"></p>\n<p> <img src=\"/../images/1251718-20200531154732786-1909345970.png\" alt=\"img\"></p>\n<p> <strong>三、DIoU(Distance-IoU)[2]</strong></p>\n<p>**1,**<strong>来源</strong></p>\n<p>DIoU要比GIou更加符合目标框回归的机制，<strong>将目标与anchor之间的距离，重叠率以及尺度都考虑进去</strong>，使得目标框回归变得更加稳定，不会像IoU和GIoU一样出现训练过程中发散等问题。论文中</p>\n<p>Distance-IoU<br> <a href=\"https://arxiv.org/pdf/1911.08287.pdf\">https://arxiv.org/pdf/1911.08287.pdf</a></p>\n<p>基于IoU和GIoU存在的问题，作者提出了两个问题：<br> \\1. 直接最小化anchor框与目标框之间的归一化距离是否可行，以达到更快的收敛速度？<br> \\2. 如何使回归在与目标框有重叠甚至包含时更准确、更快？</p>\n<p> <img src=\"/../images/1251718-20200531154844425-632049559.png\" alt=\"img\"></p>\n<p> 其中，<img src=\"/../images/1251718-20200531154815739-471463487.png\" alt=\"img\">分别代表了预测框和真实框的中心点，<img src=\"/../images/1251718-20200531154909041-22064640.png\" alt=\"img\">且代表的是计算两个中心点间的欧式距离。c代表的是能够同时包含预测框和真实框的<strong>最小闭包区域</strong>的对角线距离。</p>\n<p> <img src=\"/../images/1251718-20200531154956953-836811449.png\" alt=\"img\"></p>\n<p> <img src=\"/../images/1251718-20200531155026159-588849996.png\" alt=\"img\"></p>\n<p> DIoU中对anchor框和目标框之间的归一化距离进行了建模</p>\n<p>附：</p>\n<p>YOLOV3 DIoU GitHub项目地址<br> https&#x2F;&#x2F;github.com&#x2F;Zzh-tju&#x2F;DIoU-darknet</p>\n<p><strong>2****、优点</strong></p>\n<p>与GIoU loss类似，DIoU loss（ ）在与目标框不重叠时，仍然可以为边界框提供移动方向。</p>\n<p>DIoU loss可以直接最小化两个目标框的距离，因此比GIoU loss收敛快得多。</p>\n<p>对于包含两个框在水平方向和垂直方向上这种情况，DIoU损失可以使回归非常快，而GIoU损失几乎退化为IoU损失。</p>\n<p>DIoU还可以替换普通的IoU评价策略，应用于NMS中，使得NMS得到的结果更加合理和有效。</p>\n<p>实现代码：[3]</p>\n<p> <img src=\"/../images/1251718-20200531155128579-1060794147.png\" alt=\"img\"></p>\n<p> <img src=\"/../images/1251718-20200531155210632-956163178.png\" alt=\"img\"></p>\n<p> <strong>四、CIoU(Complete-IoU)</strong></p>\n<p>论文考虑到bbox回归三要素中的长宽比还没被考虑到计算中，因此，进一步在DIoU的基础上提出了CIoU。其惩罚项如下面公式：</p>\n<p> <img src=\"/../images/1251718-20200531155238128-675856095.png\" alt=\"img\"></p>\n<p> 实现代码：[5] </p>\n<p> <img src=\"/../images/1251718-20200531155304911-1990080212.png\" alt=\"img\"></p>\n<p> <img src=\"/../images/1251718-20200531155316931-2121130060.png\" alt=\"img\"></p>\n<p> <img src=\"/../images/1251718-20200531155335490-1670997408.png\" alt=\"img\"></p>\n<p><img src=\"/../images/image-20231219142055992.png\" alt=\"image-20231219142055992\"></p>\n<p><img src=\"/../images/image-20231219142702047.png\" alt=\"image-20231219142702047\"></p>\n<h4 id=\"L-IoU-1-IoU-L-GIoU-1-GIoU\"><a href=\"#L-IoU-1-IoU-L-GIoU-1-GIoU\" class=\"headerlink\" title=\"L(IoU)&#x3D;1-IoU,L(GIoU)&#x3D;1-GIoU\"></a>L(IoU)&#x3D;1-IoU,L(GIoU)&#x3D;1-GIoU</h4><p><img src=\"/../images/image-20231219143118491.png\" alt=\"image-20231219143118491\"></p>\n<p><img src=\"/../images/image-20231219143242889.png\" alt=\"image-20231219143242889\"></p>\n<h4 id=\"penalty-item-惩罚项\"><a href=\"#penalty-item-惩罚项\" class=\"headerlink\" title=\"penalty item 惩罚项\"></a>penalty item 惩罚项</h4><p><img src=\"/../images/image-20231219143834054.png\" alt=\"image-20231219143834054\"></p>\n<p><img src=\"/../images/image-20231219143908214.png\" alt=\"image-20231219143908214\"></p>\n<p><img src=\"/../images/image-20231219144615647.png\" alt=\"image-20231219144615647\"></p>\n<p><img src=\"/../images/image-20231219144820908.png\" alt=\"image-20231219144820908\"></p>\n<h4 id=\"用1替换\"><a href=\"#用1替换\" class=\"headerlink\" title=\"用1替换\"></a>用1替换</h4><p><img src=\"/../images/image-20231220151820969.png\" alt=\"image-20231220151820969\"><img src=\"/../images/image-20231220151851064.png\" alt=\"image-20231220151851064\"></p>\n<p><img src=\"/../images/image-20231220151924519.png\" alt=\"image-20231220151924519\"></p>\n<p><img src=\"/../images/image-20231220152401000.png\" alt=\"image-20231220152401000\"></p>\n<p><strong>YOLO训练技巧</strong></p>\n<p><strong>1.</strong></p>\n<p><img src=\"/../images/image-20231221102150112.png\" alt=\"image-20231221102150112\"></p>\n<p><em>2.</em></p>\n<p><img src=\"/../images/image-20231221102343926.png\" alt=\"image-20231221102343926\">余弦退火学习率调整的原理是根据余弦函数的形状动态地调整学习率。它通过将学习率从一个较大的初始值逐渐减小到一个较小的最小值来控制训练过程中的学习率变化。</p>\n<p>具体实现步骤如下：</p>\n<ol>\n<li>设置一个最大学习率和最小学习率的范围。</li>\n<li>定义一个周期数（通常是训练的总迭代次数）。</li>\n<li>对于每个训练迭代，计算当前周期数与总周期数之间的比例。</li>\n<li>使用余弦函数来动态计算学习率，公式如下：</li>\n</ol>\n<div class=\"language-txt\"><button title=\"Copy code\" class=\"copy\"></button><span class=\"lang\">txt</span><pre class=\"shiki material-theme-palenight\" style=\"background-color: #1a1a1a\" tabindex=\"0\"><code><span class=\"line\"><span style=\"color: #babed8\">lr = lr_min + 0.5 * (lr_max - lr_min) * (1 + cos(epoch / T_total * pi))</span></span></code></pre></div><p>其中，lr表示当前学习率，epoch表示当前周期数，T_total表示总周期数，lr_max表示最大学习率，lr_min表示最小学习率。</p>\n<ol start=\"5\">\n<li>将计算得到的学习率应用于优化器中进行权重更新。</li>\n</ol>\n<p>使用余弦退火学习率调整可以在训练初期使用较大的学习率来快速收敛，然后逐渐减小学习率以细化模型的优化过程。这种方法在训练中期能够跳出局部最优解并找到更好的全局最优解，有助于提高模型的泛化性能和训练效果。</p>\n<h4 id=\"3\"><a href=\"#3\" class=\"headerlink\" title=\"3.\"></a>3.</h4><p><img src=\"/../images/image-20231221103518038.png\" alt=\"image-20231221103518038\"></p>\n<h4 id=\"4\"><a href=\"#4\" class=\"headerlink\" title=\"4.\"></a>4.</h4><p><img src=\"/../images/image-20231221103556292.png\" alt=\"image-20231221103556292\"></p>\n<h4 id=\"5-遗传算法\"><a href=\"#5-遗传算法\" class=\"headerlink\" title=\"5.遗传算法\"></a>5.遗传算法</h4><p><img src=\"/../images/image-20231221103654283.png\" alt=\"image-20231221103654283\"></p>\n<h4 id=\"6-AMP\"><a href=\"#6-AMP\" class=\"headerlink\" title=\"6.AMP\"></a>6.AMP</h4><p><img src=\"/../images/image-20231221103725894.png\" alt=\"image-20231221103725894\"></p>\n<h4 id=\"7\"><a href=\"#7\" class=\"headerlink\" title=\"7.\"></a><img src=\"/../images/image-20231221103836498.png\" alt=\"image-20231221103836498\">7.</h4><p><img src=\"/../images/image-20231221104450473.png\" alt=\"image-20231221104450473\"></p>\n<h4 id=\"激活函数\"><a href=\"#激活函数\" class=\"headerlink\" title=\"激活函数\"></a>激活函数</h4><p><img src=\"/../images/image-20231222110059278.png\" alt=\"image-20231222110059278\"></p>\n<p><img src=\"/../images/image-20231222110124317.png\" alt=\"image-20231222110124317\"></p>\n<p><img src=\"/../images/image-20231222110150825.png\" alt=\"image-20231222110150825\"></p>\n<p><img src=\"/../images/image-20231222110400513.png\" alt=\"image-20231222110400513\"></p>\n<p><img src=\"/../images/image-20231222111314420.png\" alt=\"image-20231222111314420\"></p>\n<h4 id=\"优点：激活区域可以更多样\"><a href=\"#优点：激活区域可以更多样\" class=\"headerlink\" title=\"优点：激活区域可以更多样\"></a><img src=\"/../images/image-20231222111341197.png\" alt=\"image-20231222111341197\">优点：激活区域可以更多样</h4><p><img src=\"/../images/image-20231222111510179.png\" alt=\"image-20231222111510179\"></p>\n<h4 id=\"因此，引入了各种方法来压缩神经网络，以使大型模型可以在边缘设备上部署。模型压缩方法可以分为3类：剪枝、量化和知识蒸馏。在剪枝中，移除模型中不重要的冗余参数，以获得稀疏-紧凑的模型结构。量化涉及使用低精度数据类型表示模型的激活和权重。最后，知识蒸馏是指利用大型准确模型作为教师来训练一个小型模型，使用教师模型提供的软标签来进行训练。\"><a href=\"#因此，引入了各种方法来压缩神经网络，以使大型模型可以在边缘设备上部署。模型压缩方法可以分为3类：剪枝、量化和知识蒸馏。在剪枝中，移除模型中不重要的冗余参数，以获得稀疏-紧凑的模型结构。量化涉及使用低精度数据类型表示模型的激活和权重。最后，知识蒸馏是指利用大型准确模型作为教师来训练一个小型模型，使用教师模型提供的软标签来进行训练。\" class=\"headerlink\" title=\"因此，引入了各种方法来压缩神经网络，以使大型模型可以在边缘设备上部署。模型压缩方法可以分为3类：剪枝、量化和知识蒸馏。在剪枝中，移除模型中不重要的冗余参数，以获得稀疏&#x2F;紧凑的模型结构。量化涉及使用低精度数据类型表示模型的激活和权重。最后，知识蒸馏是指利用大型准确模型作为教师来训练一个小型模型，使用教师模型提供的软标签来进行训练。\"></a>因此，引入了各种方法来压缩神经网络，以使大型模型可以在边缘设备上部署。模型压缩方法可以分为3类：剪枝、量化和知识蒸馏。在剪枝中，移除模型中不重要的冗余参数，以获得稀疏&#x2F;紧凑的模型结构。量化涉及使用低精度数据类型表示模型的激活和权重。最后，知识蒸馏是指利用大型准确模型作为教师来训练一个小型模型，使用教师模型提供的软标签来进行训练。</h4><p>cmake_minimum_required(VERSION 3.10) project(yolov5) add_definitions(-std&#x3D;c++11) add_definitions(-DAPI_EXPORTS) option(CUDA_USE_STATIC_CUDA_RUNTIME OFF) set(CMAKE_CXX_STANDARD 11) set(CMAKE_BUILD_TYPE Debug) # TODO(Call for PR): make cmake compatible with Windows set(CMAKE_CUDA_COMPILER &#x2F;usr&#x2F;local&#x2F;cuda-12.2&#x2F;bin&#x2F;nvcc) enable_language(CUDA) # include and link dirs of cuda and tensorrt, you need adapt them if yours are different # cuda include_directories(&#x2F;usr&#x2F;local&#x2F;cuda-12.2&#x2F;include) link_directories(&#x2F;usr&#x2F;local&#x2F;cuda-12.2&#x2F;lib64) # tensorrt # TODO(Call for PR): make TRT path configurable from command line include_directories(&#x2F;home&#x2F;nvidia&#x2F;TensorRT-8.2.5.1&#x2F;include&#x2F;) link_directories(&#x2F;home&#x2F;nvidia&#x2F;TensorRT-8.2.5.1&#x2F;lib&#x2F;) include_directories(${PROJECT_SOURCE_DIR}&#x2F;src&#x2F;) include_directories(${PROJECT_SOURCE_DIR}&#x2F;plugin&#x2F;) file(GLOB_RECURSE SRCS ${PROJECT_SOURCE_DIR}&#x2F;src&#x2F;<em>.cpp ${PROJECT_SOURCE_DIR}&#x2F;src&#x2F;</em>.cu) file(GLOB_RECURSE PLUGIN_SRCS ${PROJECT_SOURCE_DIR}&#x2F;plugin&#x2F;*.cu) add_library(myplugins SHARED ${PLUGIN_SRCS}) target_link_libraries(myplugins nvinfer cudart) find_package(OpenCV) include_directories(${OpenCV_INCLUDE_DIRS}) add_executable(yolov5_det yolov5_det.cpp ${SRCS}) target_link_libraries(yolov5_det nvinfer) target_link_libraries(yolov5_det cudart) target_link_libraries(yolov5_det myplugins) target_link_libraries(yolov5_det ${OpenCV_LIBS}) add_executable(yolov5_cls yolov5_cls.cpp ${SRCS}) target_link_libraries(yolov5_cls nvinfer) target_link_libraries(yolov5_cls cudart) target_link_libraries(yolov5_cls myplugins) target_link_libraries(yolov5_cls ${OpenCV_LIBS}) add_executable(yolov5_seg yolov5_seg.cpp ${SRCS}) target_link_libraries(yolov5_seg nvinfer) target_link_libraries(yolov5_seg cudart) target_link_libraries(yolov5_seg myplugins) target_link_libraries(yolov5_seg ${OpenCV_LIBS}) 这是cmakeLists.txt文件 cmake .. 报错： zgh@zgh-Legion-Y7000P-IAH7:~&#x2F;tensorrtx&#x2F;yolov5&#x2F;build$ cmake .. CMake Error at &#x2F;usr&#x2F;local&#x2F;share&#x2F;cmake-3.27&#x2F;Modules&#x2F;CMakeDetermineCUDACompiler.cmake:277 (message):  CMAKE_CUDA_ARCHITECTURES must be non-empty if set. Call Stack (most recent call first):  CMakeLists.txt:13 (enable_language)  – Configuring incomplete, errors occurred!</p>\n<p>2024&#x2F;1&#x2F;20 11:33:33</p>\n<p>该错误提示是由于没有指定 <code>CMAKE_CUDA_ARCHITECTURES</code> 变量，需要在 <code>CMakeLists.txt</code> 中添加以下语句：</p>\n<div class=\"language-txt\"><button title=\"Copy code\" class=\"copy\"></button><span class=\"lang\">txt</span><pre class=\"shiki material-theme-palenight\" style=\"background-color: #1a1a1a\" tabindex=\"0\"><code><span class=\"line\"><span style=\"color: #babed8\"></span></span>\n<span class=\"line\"><span style=\"color: #babed8\">set(CMAKE_CUDA_ARCHITECTURES &quot;60;61;70;75&quot;)</span></span></code></pre></div><p>其中，<code>CMAKE_CUDA_ARCHITECTURES</code> 指定了支持的 CUDA 架构版本号，这里的值可以根据你的 GPU 型号进行调整。</p>\n<p>在添加上述语句后，再次运行 <code>cmake ..</code> 命令即可。</p>\n<div class=\"language-txt\"><button title=\"Copy code\" class=\"copy\"></button><span class=\"lang\">txt</span><pre class=\"shiki material-theme-palenight\" style=\"background-color: #1a1a1a\" tabindex=\"0\"><code><span class=\"line\"><span style=\"color: #babed8\">cmake_minimum_required(VERSION 3.5)</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">project(Ball_Det)</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">set(CMAKE_CXX_STANDARD 11)</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">set(Cudart_LIBS &quot;/usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudart.so&quot;)</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">set(CMAKE_CUDA_COMPILER /usr/local/cuda-12.2/bin/nvcc)</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">set(CMAKE_CUDA_ARCHITECTURES &quot;80&quot;)</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">include_directories(/usr/local/cuda-12.2/include)</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">link_directories(/usr/local/cuda-12.2/lib64)</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">enable_language(CUDA)</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">find_package(CUDA REQUIRED)</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">find_package(k4a REQUIRED)</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">find_package(OpenCV REQUIRED)</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">include_directories(</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">       Yolo</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">        $&#123;OpenCV_INCLUDE_DIRS&#125;</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">       /usr/local/cuda12.2/include</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">       $&#123;CMAKE_CURRENT_LIST_DIR&#125;</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">)</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">add_library(</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">       Yolo</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">       Yolo/yolov5.cpp</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">)</span></span>\n<span class=\"line\"><span style=\"color: #babed8\"></span></span>\n<span class=\"line\"><span style=\"color: #babed8\">cuda_add_library(myplugins SHARED ./Yolo/yololayer.cu)</span></span>\n<span class=\"line\"><span style=\"color: #babed8\"></span></span>\n<span class=\"line\"><span style=\"color: #babed8\"></span></span>\n<span class=\"line\"><span style=\"color: #babed8\">add_executable(Ball_Det</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">       Mask_Detection.cpp</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">       Yolo/yolov5.cpp</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">       main.cpp</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">       Ball_Detection.cpp</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">       Yolo/calibrator.cpp</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">)</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">target_link_libraries(</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">       Ball_Det</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">       nvinfer</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">       cudart</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">       myplugins</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">       /usr/local/cuda12.2/lib64</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">       $&#123;Cudart_LIBS&#125;</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">       $&#123;OpenCV_LIBS&#125;</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">       libk4a.so</span></span>\n<span class=\"line\"><span style=\"color: #babed8\">        )</span></span></code></pre></div>","text":" IOU —-交并比 Lou为1意味着预测边界框和地面真实边界框完全重叠。您可以为LOU设置阈值，以确定对象检测是否有效。假设您将LOU设置为0.5，在这种情况...","permalink":"/post/yolov5","photos":[],"count_time":{"symbolsCount":"9.3k","symbolsTime":"8 mins."},"categories":[],"tags":[{"name":"yolo","slug":"yolo","count":1,"path":"api/tags/yolo.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#IOU-%E2%80%94-%E4%BA%A4%E5%B9%B6%E6%AF%94\"><span class=\"toc-text\">IOU —-交并比</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#AP-MAP\"><span class=\"toc-text\">AP,MAP</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84%E5%92%8C%E7%BB%84%E4%BB%B6\"><span class=\"toc-text\">网络架构和组件</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E8%B7%AF%E5%BE%84%E8%81%9A%E5%90%88%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9D%97\"><span class=\"toc-text\">路径聚合网络模块</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#Focus%E5%A4%84%E7%90%86%E6%A8%A1%E5%9D%97\"><span class=\"toc-text\">Focus处理模块</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E7%A9%BA%E9%97%B4%E9%87%91%E5%AD%97%E5%A1%94%E6%B1%A0%E5%8C%96%E6%A8%A1%E5%9D%97\"><span class=\"toc-text\">空间金字塔池化模块</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E8%B7%A8%E9%98%B6%E6%AE%B5%E5%B1%80%E9%83%A8%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9D%97\"><span class=\"toc-text\">跨阶段局部网络模块</span></a></li></ol></li></ol></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#IoU%E3%80%81GIoU%E3%80%81DIoU%E3%80%81CIoU%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0\"><span class=\"toc-text\">     IoU、GIoU、DIoU、CIoU损失函数         </span></a></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%BB%BB%E5%8A%A1%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%94%B1Classificition-Loss%E5%92%8CBounding-Box-Regeression-Loss%E4%B8%A4%E9%83%A8%E5%88%86%E6%9E%84%E6%88%90%E3%80%82%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%BB%BB%E5%8A%A1%E4%B8%AD%E8%BF%91%E5%87%A0%E5%B9%B4%E6%9D%A5Bounding-Box-Regression-Loss-Function%E7%9A%84%E6%BC%94%E8%BF%9B%E8%BF%87%E7%A8%8B%EF%BC%8C%E5%85%B6%E6%BC%94%E8%BF%9B%E8%B7%AF%E7%BA%BF%E6%98%AF\"><span class=\"toc-text\">目标检测任务的损失函数由Classificition Loss和Bounding Box Regeression Loss两部分构成。目标检测任务中近几年来Bounding Box Regression Loss Function的演进过程，其演进路线是</span></a></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\"><span class=\"toc-text\"></span></a></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E4%B8%80%E3%80%81IOU-Intersection-over-Union\"><span class=\"toc-text\">一、IOU(Intersection over Union)</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#L-IoU-1-IoU-L-GIoU-1-GIoU\"><span class=\"toc-text\">L(IoU)&#x3D;1-IoU,L(GIoU)&#x3D;1-GIoU</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#penalty-item-%E6%83%A9%E7%BD%9A%E9%A1%B9\"><span class=\"toc-text\">penalty item 惩罚项</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E7%94%A81%E6%9B%BF%E6%8D%A2\"><span class=\"toc-text\">用1替换</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#3\"><span class=\"toc-text\">3.</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#4\"><span class=\"toc-text\">4.</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#5-%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95\"><span class=\"toc-text\">5.遗传算法</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#6-AMP\"><span class=\"toc-text\">6.AMP</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#7\"><span class=\"toc-text\">7.</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0\"><span class=\"toc-text\">激活函数</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E4%BC%98%E7%82%B9%EF%BC%9A%E6%BF%80%E6%B4%BB%E5%8C%BA%E5%9F%9F%E5%8F%AF%E4%BB%A5%E6%9B%B4%E5%A4%9A%E6%A0%B7\"><span class=\"toc-text\">优点：激活区域可以更多样</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E5%9B%A0%E6%AD%A4%EF%BC%8C%E5%BC%95%E5%85%A5%E4%BA%86%E5%90%84%E7%A7%8D%E6%96%B9%E6%B3%95%E6%9D%A5%E5%8E%8B%E7%BC%A9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%8C%E4%BB%A5%E4%BD%BF%E5%A4%A7%E5%9E%8B%E6%A8%A1%E5%9E%8B%E5%8F%AF%E4%BB%A5%E5%9C%A8%E8%BE%B9%E7%BC%98%E8%AE%BE%E5%A4%87%E4%B8%8A%E9%83%A8%E7%BD%B2%E3%80%82%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E6%96%B9%E6%B3%95%E5%8F%AF%E4%BB%A5%E5%88%86%E4%B8%BA3%E7%B1%BB%EF%BC%9A%E5%89%AA%E6%9E%9D%E3%80%81%E9%87%8F%E5%8C%96%E5%92%8C%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F%E3%80%82%E5%9C%A8%E5%89%AA%E6%9E%9D%E4%B8%AD%EF%BC%8C%E7%A7%BB%E9%99%A4%E6%A8%A1%E5%9E%8B%E4%B8%AD%E4%B8%8D%E9%87%8D%E8%A6%81%E7%9A%84%E5%86%97%E4%BD%99%E5%8F%82%E6%95%B0%EF%BC%8C%E4%BB%A5%E8%8E%B7%E5%BE%97%E7%A8%80%E7%96%8F-%E7%B4%A7%E5%87%91%E7%9A%84%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84%E3%80%82%E9%87%8F%E5%8C%96%E6%B6%89%E5%8F%8A%E4%BD%BF%E7%94%A8%E4%BD%8E%E7%B2%BE%E5%BA%A6%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E8%A1%A8%E7%A4%BA%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%BF%80%E6%B4%BB%E5%92%8C%E6%9D%83%E9%87%8D%E3%80%82%E6%9C%80%E5%90%8E%EF%BC%8C%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F%E6%98%AF%E6%8C%87%E5%88%A9%E7%94%A8%E5%A4%A7%E5%9E%8B%E5%87%86%E7%A1%AE%E6%A8%A1%E5%9E%8B%E4%BD%9C%E4%B8%BA%E6%95%99%E5%B8%88%E6%9D%A5%E8%AE%AD%E7%BB%83%E4%B8%80%E4%B8%AA%E5%B0%8F%E5%9E%8B%E6%A8%A1%E5%9E%8B%EF%BC%8C%E4%BD%BF%E7%94%A8%E6%95%99%E5%B8%88%E6%A8%A1%E5%9E%8B%E6%8F%90%E4%BE%9B%E7%9A%84%E8%BD%AF%E6%A0%87%E7%AD%BE%E6%9D%A5%E8%BF%9B%E8%A1%8C%E8%AE%AD%E7%BB%83%E3%80%82\"><span class=\"toc-text\">因此，引入了各种方法来压缩神经网络，以使大型模型可以在边缘设备上部署。模型压缩方法可以分为3类：剪枝、量化和知识蒸馏。在剪枝中，移除模型中不重要的冗余参数，以获得稀疏&#x2F;紧凑的模型结构。量化涉及使用低精度数据类型表示模型的激活和权重。最后，知识蒸馏是指利用大型准确模型作为教师来训练一个小型模型，使用教师模型提供的软标签来进行训练。</span></a></li></ol>","author":{"name":"Zgh","slug":"blog-author","avatar":"","link":"/","description":"","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"mapped":true,"hidden":false,"prev_post":{"title":"Esp32","uid":"2da31e9118a69e38c27812cfeb6f1fe9","slug":"Esp32","date":"2023-12-20T08:57:19.000Z","updated":"2023-12-28T08:01:29.275Z","comments":true,"path":"api/articles/Esp32.json","keywords":null,"cover":[],"text":" ","permalink":"/post/Esp32","photos":[],"count_time":{"symbolsCount":1,"symbolsTime":"1 mins."},"categories":[],"tags":[{"name":"esp32","slug":"esp32","count":1,"path":"api/tags/esp32.json"}],"author":{"name":"Zgh","slug":"blog-author","avatar":"","link":"/","description":"","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}}},"next_post":{"title":"deeplearning","uid":"2dd54cd4b432aa48fbfa03c6312b571a","slug":"deeplearning","date":"2023-12-16T02:57:26.000Z","updated":"2024-04-14T12:10:26.463Z","comments":true,"path":"api/articles/deeplearning.json","keywords":null,"cover":[],"text":"在 Pandas 中，.apply() 是用于对 DataFrame 或 Series 中的元素应用指定函数的方法。 对于 DataFrame，.apply()...","permalink":"/post/deeplearning","photos":[],"count_time":{"symbolsCount":"1.1k","symbolsTime":"1 mins."},"categories":[],"tags":[],"author":{"name":"Zgh","slug":"blog-author","avatar":"","link":"/","description":"","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}}}}