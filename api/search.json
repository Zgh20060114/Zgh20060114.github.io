[{"id":"f0da541dcd6df6287dc7b7bcefa5bded","title":"yolov5","content":"\n\nIOU —-交并比\nLou为1意味着预测边界框和地面真实边界框完全重叠。您可以为LOU设置阈值，以确定对象检测是否有效。假设您将LOU设置为0.5，在这种情况下。·如果LOU≥为0.5，则将目标检测归类为真阳性(TP)。如果LOU&lt;0.5，则为错误检测，并将其归类为假阳性(FP)。当图像中存在地面真实且模型未能检测到目标时，分类。作为假阴性(FN)。真负片(TN)：TN是我们没有预测到物体的图像的每一部分。度量对于目标检测没有用处，因此我们忽略TN。\n\nAP,MAP\n\n\n   \n \n网络架构和组件单阶段检测器：\n\nyolov5：（没有划出专门的颈部Neck）\n\ngit clone https://github.moeyy.xyz/https://github.com/ultralytics/yolov5.git![image-20231217123703969](../images/image-20231217123703969.png)\n\nnc: 80：这个参数表示模型分类数量（number of classes），默认为 80，对应着 COCO 数据集。\ndepth_multiple: 0.33：这个参数表示模型深度相对于基础版本的倍数。在 YOLOv5 中，有 S、M、L 和 X 四个版本，其中 S 为基础版本，即 depth_multiple: 1.0，而 M、L 和 X 版本为在此基础上分别加深了一定的层数。而 depth_multiple: 0.33 表示在 S 版本的基础上，深度缩小了 3 倍，即变成了 depth_multiple: 0.33 × 3 &#x3D; 0.99。\nwidth_multiple: 0.50：这个参数表示模型通道宽度相对于基础版本的倍数。与 depth_multiple 类似，S 版本的 width_multiple 是 1.0，而 M、L 和 X 版本则在此基础上分别扩大了一定的倍数。\nanchors：这是一个锚点数组，用于定义不同尺度下的 anchor boxes。YOLOv5 中使用了三个不同的尺度，每个尺度使用三个不同的 anchor boxes。这些锚点大小是相对于输入图像的，因此不同尺度下的大小会有所差别。\nbackbone：这一部分定义了模型的骨干网络（backbone），包括卷积层、批归一化层和激活函数等。YOLOv5 使用了 CSPDarknet53 这个网络作为基础骨干网络，并在此基础上进行改进。具体而言，YOLOv5 增加了空间注意力机制和SPP模块，以增强特征表达能力。\nhead：这一部分定义了模型的检测头（detection head），包括检测网络和分类网络。YOLOv5 中的检测网络采用了YOLOv3中的FPN结构，并在此基础上加入了PANet模块和SAM模块，以提高检测性能。\n\n序列数据的不同采样方法（随机采样和顺序分区）会导致隐状态初始化的差异，原因如下：\n\n随机采样： 在随机采样中，我们从序列数据中随机选择一个序列作为训练样本。这意味着每次训练时，我们都使用不同的序列作为输入。由于每个序列可能具有不同的上下文和语义信息，模型在每次训练时都需要重新适应不同的序列特征。因此，随机采样会导致隐状态的初始化与之前的训练批次存在一定差异。\n顺序分区： 在顺序分区中，我们按顺序依次读取序列数据进行训练。这意味着模型在每个训练批次中都会接收到相邻的序列数据。由于相邻的序列通常具有相似的上下文和语义信息，模型可以利用之前批次的隐藏状态来帮助理解当前批次的序列。因此，顺序分区会导致隐状态的初始化与之前的训练批次存在一定的相关性。\n\n不同的隐状态初始化差异可能会对模型的训练和预测产生影响。随机采样可以增加数据的多样性，帮助模型更好地适应不同的序列特征。然而，随机采样可能也会引入一些噪声，导致训练过程更加不稳定。顺序分区可以利用相邻序列之间的相关性，帮助模型更好地捕捉到序列的上下文信息。然而，顺序分区可能会限制模型对不同序列特征的适应能力。\n困惑度（perplexity）是自然语言处理中常用的一个评价指标，主要用于衡量语言模型的预测性能。困惑度越低，表示模型的预测能力越好。\n在自然语言处理中，我们通常使用语言模型来计算文本序列的概率。给定一个文本序列 $W&#x3D;w_1,w_2,…,w_n$，其概率可以表示为：\n$$P(W)&#x3D;P(w_1)\\times P(w_2|w_1) \\times … \\times P(w_n|w_1,w_2,…,w_{n-1})$$\n其中，$P(w_i|w_1,w_2,…,w_{i-1})$ 表示在已知前面 $i-1$ 个词的情况下，第 $i$ 个词的概率。语言模型的目标就是学习这种条件概率分布。在模型训练过程中，我们通常会使用最大似然估计法来估计模型参数。\n困惑度是一个数值指标，表示用当前语言模型对一个测试集进行预测时所得到的困惑程度。具体而言，如果测试集包含 $N$ 个词，我们可以计算出每个词的概率 $P(w_i)$，然后将这些概率求倒数并取对数，即：\n$$\\log \\frac{1}{P(w_1)}+\\log \\frac{1}{P(w_2|w_1)}+…+\\log \\frac{1}{P(w_N|w_1,w_2,…,w_{N-1})}$$\n然后，我们可以将上述结果除以测试集中的词数 $N$，得到平均困惑度。具体而言，平均困惑度的计算公式如下：\n$$\\text{Perplexity}&#x3D;exp\\left(-\\frac{1}{N}\\sum_{i&#x3D;1}^{N}\\log P(w_i)\\right)$$\n例如，如果我们有一个包含100个句子的测试集，其中总共包含1000个词，我们可以使用语言模型来预测每个词的概率，并计算出平均困惑度。假设我们的模型预测准确率较高，平均每个词的概率为0.9，则平均困惑度为：\n$$exp\\left(-\\frac{1}{1000}\\sum_{i&#x3D;1}^{1000}\\log 0.9\\right) \\approx 2.15$$\n这表示我们的模型对测试集中的文本序列进行预测时，每个词的平均困惑度为2.15。如果我们使用一个更好的语言模型，其困惑度可能会更低。\n用困惑度来评价模型确保了不同长度的序列具有可比性\n","slug":"yolov5","date":"2023-12-16T12:04:59.000Z","categories_index":"","tags_index":"yolo","author_index":"Zgh"},{"id":"2dd54cd4b432aa48fbfa03c6312b571a","title":"deeplearning","content":"在 Pandas 中，.apply() 是用于对 DataFrame 或 Series 中的元素应用指定函数的方法。\n对于 DataFrame，.apply() 可以在行或列方向上应用函数。语法如下：\ntxtDataFrame.apply(func, axis=0)\nfunc 是要应用的函数，可以是一个已定义的函数，也可以是一个匿名函数（如 lambda 函数）。\naxis 是指定应用函数的方向，默认为 0，表示按列应用函数；设置为 1 则表示按行应用函数。\n\n对于 Series，.apply() 仅能在元素级别上应用函数，无需指定应用方向。语法如下：\ntxtSeries.apply(func)\nfunc 是要应用的函数，可以是一个已定义的函数，也可以是一个匿名函数（如 lambda 函数）。\n\n在上述代码中，.apply(lambda x: (x - x.mean()) / (x.std())) 就是将匿名函数 lambda x: (x - x.mean()) / (x.std()) 应用到 DataFrame 或 Series 中的每个元素上。结果是对 DataFrame 或 Series 中的每个元素进行标准化计算，并返回处理后的结果\n","slug":"deeplearning","date":"2023-12-16T02:57:26.000Z","categories_index":"","tags_index":"","author_index":"Zgh"},{"id":"61251d8b73997ae6479adce2ddc93d34","title":"Pytorch","content":"在 Python 中，lambda 是用来创建匿名函数的关键字。所谓匿名函数，即没有显式定义函数名的函数，通常用于需要临时定义简单函数的场景。\nlambda 函数的语法如下：\ntxtlambda arguments: expression其中：\n\narguments 是函数的参数，可以有多个参数，用逗号隔开。\nexpression 是函数的返回值计算表达式。\n\nlambda 函数通常用于需要一个函数，但是又不想正式定义一个函数的场景，比如作为其他函数的参数传递进去，或者在一些函数式编程的场景中使用。\nparams &#x3D; [W_xh, W_hh, b_h, W_hq, b_q]for param in params:        param.requires_grad_(True)\n目的是告诉 PyTorch 在模型训练过程中需要计算这些参数的梯度，并且在反向传播时对其进行更新。\nXt*Wxh + Ht−1*Whh=cat(Xt,Ht−1)*cat(Wxh,Whh)\ntorch.matmul(X, W_xh) + torch.matmul(H, W_hh)\ntorch.matmul(torch.cat((X, H), 1), torch.cat((W_xh, W_hh), 0))\n当一个类实现了 __call__ 方法时，它的实例对象可以像函数一样进行调用。这意味着你可以使用实例对象作为函数来调用，就好像调用一个函数一样。\n例如，假设有一个类 MyClass，并且实现了 __call__ 方法：\ntxtpython\nclass MyClass:\n    def __call__(self, x):\n        print(&quot;Calling MyClass with argument:&quot;, x)现在，你可以创建一个 MyClass 的实例，并将其作为函数进行调用：\ntxtpython\nobj = MyClass()\nobj(10)输出结果将是：\ntxt\nCalling MyClass with argument: 10所以class RNNModelScratch: #@save&quot;&quot;&quot;从零开始实现的循环神经网络模型&quot;&quot;&quot;def __init__(self, vocab_size, num_hiddens, device,get_params, init_state, forward_fn):self.vocab_size, self.num_hiddens = vocab_size, num_hiddensself.params = get_params(vocab_size, num_hiddens, device)self.init_state, self.forward_fn = init_state, forward_fndef __call__(self, X, state):X = F.one_hot(X.T, self.vocab_size).type(torch.float32)return self.forward_fn(X, state, self.params)\n可以这样调用：\nnet = RNNModelScratch(len(vocab), num_hiddens, d2l.try_gpu(), get_params,init_rnn_state, rnn)\nY, new_state = net(X.to(d2l.try_gpu()), state)\n","slug":"Pytorch","date":"2023-12-15T14:50:19.000Z","categories_index":"","tags_index":"","author_index":"Zgh"},{"id":"b9663f58f18133b35bfe243f3e916a80","title":"Hello World","content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.\nQuick StartCreate a new postbash$ hexo new &quot;My New Post&quot;More info: Writing\nRun serverbash$ hexo serverMore info: Server\nGenerate static filesbash$ hexo generateMore info: Generating\nDeploy to remote sitesbash$ hexo deployMore info: Deployment\n","slug":"hello-world","date":"2023-12-15T14:27:11.801Z","categories_index":"","tags_index":"","author_index":"Zgh"}]