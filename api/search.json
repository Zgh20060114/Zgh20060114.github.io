[{"id":"a844e8402236c25b1a86bfe3611488cf","title":"Embedded","content":"为什么原理图里的stm32f4的VCAP1引脚要接一个4.7微法的电容，再接地呢\n2023&#x2F;12&#x2F;25 08:23:32\nSTM32F4芯片的VCAP1引脚是芯片内部电容的引出端，用于连接外部电容以提供给芯片内部振荡电路使用。在使用高速外设时，由于这些外设需要非常稳定的电源供应，因此需要在VCAP1引脚与地之间连接一个电容来提高芯片的稳定性。\n具体来说，连接4.7微法的电容可以起到平滑电压、提高电源噪声抑制能力等作用，从而提高芯片的工作稳定性和可靠性。此外，需要注意电容的选型要符合芯片手册的要求，不能超出规定的范围。\n2023&#x2F;12&#x2F;25 08:26:15芯片内部还有电容吗\n2023&#x2F;12&#x2F;25 08:26:20\n是的，芯片内部通常会包含一些电容。这些内部电容主要用于稳定芯片的电源供应和进行滤波。它们的作用包括：\ntxt电源滤波：在芯片内部集成了一些小型电容器，用于过滤电源中的噪声和干扰，确保芯片正常运行所需的电源干净和稳定。\n\n芯片电源稳定：芯片内部可能还包含一些供电相关的电容，用于提供瞬态响应和电源稳定性，以确保芯片在各种工作条件下都能正常工作。这些内部电容一般由芯片制造商在设计阶段根据具体需求添加，并在芯片手册或参考设计中进行了详细说明。在使用芯片时，我们不需要特别处理这些内部电容，只需按照芯片手册的要求合理连接外部电容即可。\n2023&#x2F;12&#x2F;25 08:26:41那为什么还要连接外部电容\n2023&#x2F;12&#x2F;25 08:26:48\n连接外部电容主要是为了进一步提高芯片的电源稳定性和抑制电磁干扰。虽然芯片内部已经包含了一些电容，但这些电容的容值较小且位置相对固定，可能无法满足某些特殊应用或高要求的电源稳定性。\n通过在VCAP1引脚与地之间连接外部电容，可以实现以下效果：\ntxt改善电源噪声抑制能力：外部电容可以提供更大的容值，能够更好地滤除电源中的高频噪声和纹波，从而提供更稳定的电源供应。\n\n提供额外的瞬态响应：外部电容能够快速响应芯片工作状态的变化，提供更好的瞬态响应能力，尤其在高速外设工作时，可以提供更稳定的电源给这些外设。\n\n抑制电磁干扰：外部电容能够帮助减少电源线上的电磁辐射和接收到的外界干扰信号，提高系统的抗干扰能力，保证芯片的正常运行。需要注意的是，具体的外部电容选型和连接方式应根据芯片厂商提供的参考设计或芯片手册的建议进行，以确保电路的稳定性和可靠性。\n\n\n配置通道    具有检测正反插…..等\n\n边带使用实现…..\n\n\n\n\n\nOSC32IN引脚，用于连接外部低速晶体振荡器（32.768 kHz）。这种振荡器通常用于实时时钟（RTC)的时基。\nOSC_IN引脚，用于连接外部高速晶体振荡器（通常在4 MHz到26 MHz之间）。这种振荡器通常用于系统时钟（SYSCLK）的时基。\nVBAT代表电池供电引脚（Voltage Battery）。VBAT引脚用于连接外部备用电池，通常是一个小型电池，以提供芯片的备份电源。\nVCAP1是指STM32系列芯片上的一个电容滤波器引脚，用于为芯片内部提供稳定的电源环境,用于去除芯片电源中的高频噪声和波动。\n\n在电路设计中，加入0欧电阻（也称为短接电阻）有以下几个常见的目的：\n\n电路连接：0欧电阻可以作为电路的连接点，将不同信号线或电源线连接在一起。它可以提供良好的电气连接，并保持信号或电流的连续性。\n信号分配：在复杂的电路板上，使用0欧电阻可以方便地将信号线分配到不同的位置。通过添加或移除0欧电阻，可以改变信号路径，以满足电路的布局要求。\n调试和测试：在调试和测试电路时，加入0欧电阻可以方便地连接测试仪器或测量点。它可以用作测试点或测量点的连接线，以便在需要时轻松接入测试设备。\n阻抗匹配：在高频电路设计中，0欧电阻可以用于阻抗匹配。通过选择适当的阻值，可以使电路中的阻抗更好地匹配，以最大限度地传输信号或功率。\n\n需要注意的是，加入0欧电阻并不会引入电阻本身的阻尼效应。它的主要作用是提供连接和分配功能，而不会对电路的整体电性能产生实质性的影响。\n\nTPS3823-33DBVR的MR引脚是干什么的\n1&#x2F;3&#x2F;2024, 5:11:40 PM\nTPS3823-33DBVR是一款超低功耗、微型化的复位监测器。MR引脚是器件的主复位引脚，用于手动地对器件进行复位操作。\n当MR引脚被拉低时，TPS3823-33DBVR会立即进行复位操作，并将RST引脚拉低，使被监测的系统或芯片得到重新启动。此外，当MR引脚保持低电平时，TPS3823-33DBVR将始终保持在复位状态，直到MR引脚上升到高电平。\n需要注意的是，MR引脚应该在正常工作时保持高电平，并通过一个合适的电阻接到正电源。当需要手动复位时，可以通过连接一个按钮或开关来将MR引脚拉低，从而触发复位操作。\n总之，TPS3823-33DBVR的MR引脚是用于手动复位操作的引脚，当MR引脚被拉低时，TPS3823-33DBVR会进行复位操作，从而实现对被监测系统或芯片的重新启动。\n1&#x2F;3&#x2F;2024, 5:12:51 PM\nTPS3823-33DBVR的WDI引脚是干什么的\n1&#x2F;3&#x2F;2024, 5:13:13 PM\nTPS3823-33DBVR的WDI引脚是用于喂狗定时器（Watchdog Timer）的输入引脚。\n喂狗定时器是一种用于监控系统运行状态的计时器。它需要周期性地接收一个喂狗脉冲来防止系统出现故障或死锁。如果在一定的时间内没有接收到喂狗脉冲，喂狗定时器会触发复位操作，使被监控的系统重新启动。\n在TPS3823-33DBVR中，WDI引脚用于接收外部喂狗脉冲。当WDI引脚接收到一个高电平脉冲时，喂狗定时器会被重置，系统继续正常运行。如果在一定时间内未接收到喂狗脉冲，喂狗定时器将超时，并触发复位操作。\n通过连接一个外部的喂狗脉冲源（例如微处理器的GPIO引脚），可以周期性地向TPS3823-33DBVR的WDI引脚提供喂狗脉冲。这样可以确保系统持续运行，并在出现故障或死锁时进行自动复位，提高系统的可靠性和稳定性。\n总之，TPS3823-33DBVR的WDI引脚用于接收外部喂狗脉冲，用于喂狗定时器的重置操作，以确保系统正常运行并防止故障或死锁。\nMPU6050https://blog.csdn.net/qq_52251819/article/details/134957492\n\nAHT21\n\nVdd_IO和Vdd是两个不同的电源引脚，常见于集成电路或芯片中。\n\nVdd（也称为Vcc）是供电引脚，用于提供主要的电源电压给芯片或集成电路的内部电路。它通常连接到正电源（例如+3.3V或+5V），以供应芯片内部的数字和模拟电路。\nVdd_IO（也称为Vcc_IO或VddIO）是IO供电引脚，用于提供给输入输出（IO）电路的电源电压。它通常连接到较低的电压（例如+1.8V或+3.3V），用于驱动芯片的输入和输出引脚。\n\n区别：\n\n功能：Vdd提供整个芯片的电源，包括内部逻辑、处理器核心等。Vdd_IO则专门提供给IO电路使用。\n电压级别：Vdd通常连接到较高的电压，而Vdd_IO连接到较低的电压。这是为了在现代集成电路中实现低功耗设计和电压兼容性。\n\n","slug":"Embedded","date":"2023-12-25T00:29:56.000Z","categories_index":"","tags_index":"Embedded","author_index":"Zgh"},{"id":"2da31e9118a69e38c27812cfeb6f1fe9","title":"Esp32","content":"\n","slug":"Esp32","date":"2023-12-20T08:57:19.000Z","categories_index":"","tags_index":"esp32","author_index":"Zgh"},{"id":"f0da541dcd6df6287dc7b7bcefa5bded","title":"yolov5","content":"\n\nIOU —-交并比\nLou为1意味着预测边界框和地面真实边界框完全重叠。您可以为LOU设置阈值，以确定对象检测是否有效。假设您将LOU设置为0.5，在这种情况下。·如果LOU≥为0.5，则将目标检测归类为真阳性(TP)。如果LOU&lt;0.5，则为错误检测，并将其归类为假阳性(FP)。当图像中存在地面真实且模型未能检测到目标时，分类。作为假阴性(FN)。真负片(TN)：TN是我们没有预测到物体的图像的每一部分。度量对于目标检测没有用处，因此我们忽略TN。\n\nAP,MAP\n\n\n   \n \n网络架构和组件单阶段检测器：\n\nyolov5：（没有划出专门的颈部Neck）\n\ngit clone https://github.moeyy.xyz/https://github.com/ultralytics/yolov5.git\n\n\nnc: 80：这个参数表示模型分类数量（number of classes），默认为 80，对应着 COCO 数据集。\ndepth_multiple: 0.33：这个参数表示模型深度相对于基础版本的倍数。在 YOLOv5 中，有 S、M、L 和 X 四个版本，其中 S 为基础版本，即 depth_multiple: 1.0，而 M、L 和 X 版本为在此基础上分别加深了一定的层数。而 depth_multiple: 0.33 表示在 S 版本的基础上，深度缩小了 3 倍，即变成了 depth_multiple: 0.33 × 3 &#x3D; 0.99。\nwidth_multiple: 0.50：这个参数表示模型通道宽度相对于基础版本的倍数。与 depth_multiple 类似，S 版本的 width_multiple 是 1.0，而 M、L 和 X 版本则在此基础上分别扩大了一定的倍数。\nanchors：这是一个锚点数组，用于定义不同尺度下的 anchor boxes。YOLOv5 中使用了三个不同的尺度，每个尺度使用三个不同的 anchor boxes。这些锚点大小是相对于输入图像的，因此不同尺度下的大小会有所差别。\nbackbone：这一部分定义了模型的骨干网络（backbone），包括卷积层、批归一化层和激活函数等。YOLOv5 使用了 CSPDarknet53 这个网络作为基础骨干网络，并在此基础上进行改进。具体而言，YOLOv5 增加了空间注意力机制和SPP模块，以增强特征表达能力。\nhead：这一部分定义了模型的检测头（detection head），包括检测网络和分类网络。YOLOv5 中的检测网络采用了YOLOv3中的FPN结构，并在此基础上加入了PANet模块和SAM模块，以提高检测性能。\n\n序列数据的不同采样方法（随机采样和顺序分区）会导致隐状态初始化的差异，原因如下：\n\n随机采样： 在随机采样中，我们从序列数据中随机选择一个序列作为训练样本。这意味着每次训练时，我们都使用不同的序列作为输入。由于每个序列可能具有不同的上下文和语义信息，模型在每次训练时都需要重新适应不同的序列特征。因此，随机采样会导致隐状态的初始化与之前的训练批次存在一定差异。\n顺序分区： 在顺序分区中，我们按顺序依次读取序列数据进行训练。这意味着模型在每个训练批次中都会接收到相邻的序列数据。由于相邻的序列通常具有相似的上下文和语义信息，模型可以利用之前批次的隐藏状态来帮助理解当前批次的序列。因此，顺序分区会导致隐状态的初始化与之前的训练批次存在一定的相关性。\n\n不同的隐状态初始化差异可能会对模型的训练和预测产生影响。随机采样可以增加数据的多样性，帮助模型更好地适应不同的序列特征。然而，随机采样可能也会引入一些噪声，导致训练过程更加不稳定。顺序分区可以利用相邻序列之间的相关性，帮助模型更好地捕捉到序列的上下文信息。然而，顺序分区可能会限制模型对不同序列特征的适应能力。\n困惑度（perplexity）是自然语言处理中常用的一个评价指标，主要用于衡量语言模型的预测性能。困惑度越低，表示模型的预测能力越好。\n在自然语言处理中，我们通常使用语言模型来计算文本序列的概率。给定一个文本序列 $W&#x3D;w_1,w_2,…,w_n$，其概率可以表示为：\n$$P(W)&#x3D;P(w_1)\\times P(w_2|w_1) \\times … \\times P(w_n|w_1,w_2,…,w_{n-1})$$\n其中，$P(w_i|w_1,w_2,…,w_{i-1})$ 表示在已知前面 $i-1$ 个词的情况下，第 $i$ 个词的概率。语言模型的目标就是学习这种条件概率分布。在模型训练过程中，我们通常会使用最大似然估计法来估计模型参数。\n困惑度是一个数值指标，表示用当前语言模型对一个测试集进行预测时所得到的困惑程度。具体而言，如果测试集包含 $N$ 个词，我们可以计算出每个词的概率 $P(w_i)$，然后将这些概率求倒数并取对数，即：\n$$\\log \\frac{1}{P(w_1)}+\\log \\frac{1}{P(w_2|w_1)}+…+\\log \\frac{1}{P(w_N|w_1,w_2,…,w_{N-1})}$$\n然后，我们可以将上述结果除以测试集中的词数 $N$，得到平均困惑度。具体而言，平均困惑度的计算公式如下：\n$$\\text{Perplexity}&#x3D;exp\\left(-\\frac{1}{N}\\sum_{i&#x3D;1}^{N}\\log P(w_i)\\right)$$\n例如，如果我们有一个包含100个句子的测试集，其中总共包含1000个词，我们可以使用语言模型来预测每个词的概率，并计算出平均困惑度。假设我们的模型预测准确率较高，平均每个词的概率为0.9，则平均困惑度为：\n$$exp\\left(-\\frac{1}{1000}\\sum_{i&#x3D;1}^{1000}\\log 0.9\\right) \\approx 2.15$$\n这表示我们的模型对测试集中的文本序列进行预测时，每个词的平均困惑度为2.15。如果我们使用一个更好的语言模型，其困惑度可能会更低。\n用困惑度来评价模型确保了不同长度的序列具有可比性\n路径聚合网络模块\nFocus处理模块\n空间金字塔池化模块\n跨阶段局部网络模块\n\n​\t\n     IoU、GIoU、DIoU、CIoU损失函数         IoU、GIoU、DIoU、CIoU损失函数\n目标检测任务的损失函数由Classificition Loss和Bounding Box Regeression Loss两部分构成。目标检测任务中近几年来Bounding Box Regression Loss Function的演进过程，其演进路线是一、IOU(Intersection over Union)1. 特性(优点)\nIoU就是我们所说的交并比，是目标检测中最常用的指标，在anchor-based的方法。作用不仅用来确定正样本和负样本，还可以用来评价输出框（predict box）和ground-truth的距离。\n \n \\1. 可以说它可以反映预测检测框与真实检测框的检测效果。\n \\2. 还有一个很好的特性就是尺度不变性，也就是对尺度不敏感（scale invariant）， 在regression任务中，判断predict box和gt的距离最直接的指标就是IoU。**(**满足非负性；同一性；对称性；三角不等性)\n\n 2. 作为损失函数会出现的问题(缺点)\n\\1. 如果两个框没有相交，根据定义，IoU&#x3D;0，不能反映两者的距离大小（重合度）。同时因为loss&#x3D;0，没有梯度回传，无法进行学习训练。\n \\2. IoU无法精确的反映两者的重合度大小。如下图所示，三种情况IoU都相等，但看得出来他们的重合度是不一样的，左边的图回归的效果最好，右边的最差。\n \n 二、GIOU(Generalized Intersection over Union)\n1****、来源\n在CVPR2019中，论文\n《Generalized Intersection over Union: A Metric and A Loss for Bounding Box Regression》 https:arxiv.org&#x2F;abs&#x2F;1902.09630\n提出了GIoU的思想。由于IoU是比值的概念，对目标物体的scale是不敏感的。然而检测任务中的BBox的回归损失(MSE loss, l1-smooth loss等）优化和IoU优化不是完全等价的，而且 Ln 范数对物体的scale也比较敏感，IoU无法直接优化没有重叠的部分。\n \n 这篇论文提出可以直接把IoU设为回归的loss。\n \n 上面公式的意思是：先计算两个框的最小闭包区域面积_ _(通俗理解：同时包含了预测框和真实框的最小框的面积)，再计算出IoU，再计算闭包区域中不属于两个框的区域占闭包区域的比重，最后用IoU减去这个比重得到GIoU。\n 附：https://github.com/generalized-iou/g-darknet\n2****、 特性[1]\n与IoU相似，GIoU也是一种距离度量，作为损失函数的话， ,满足损失函数的基本要求\nGIoU对scale不敏感\nGIoU是IoU的下界，在两个框无线重合的情况下，IoU&#x3D;GIoU\nIoU取值[0,1]，但GIoU有对称区间，取值范围[-1,1]。在两者重合的时候取最大值1，在两者无交集且无限远的时候取最小值-1，因此GIoU是一个非常好的距离度量指标。\n与IoU只关注重叠区域不同，GIoU****不仅关注重叠区域，还关注其他的非重合区域，能更好的反映两者的重合度。\n \n \n 三、DIoU(Distance-IoU)[2]\n**1,**来源\nDIoU要比GIou更加符合目标框回归的机制，将目标与anchor之间的距离，重叠率以及尺度都考虑进去，使得目标框回归变得更加稳定，不会像IoU和GIoU一样出现训练过程中发散等问题。论文中\nDistance-IoU https://arxiv.org/pdf/1911.08287.pdf\n基于IoU和GIoU存在的问题，作者提出了两个问题： \\1. 直接最小化anchor框与目标框之间的归一化距离是否可行，以达到更快的收敛速度？ \\2. 如何使回归在与目标框有重叠甚至包含时更准确、更快？\n \n 其中，分别代表了预测框和真实框的中心点，且代表的是计算两个中心点间的欧式距离。c代表的是能够同时包含预测框和真实框的最小闭包区域的对角线距离。\n \n \n DIoU中对anchor框和目标框之间的归一化距离进行了建模\n附：\nYOLOV3 DIoU GitHub项目地址 https&#x2F;&#x2F;github.com&#x2F;Zzh-tju&#x2F;DIoU-darknet\n2****、优点\n与GIoU loss类似，DIoU loss（ ）在与目标框不重叠时，仍然可以为边界框提供移动方向。\nDIoU loss可以直接最小化两个目标框的距离，因此比GIoU loss收敛快得多。\n对于包含两个框在水平方向和垂直方向上这种情况，DIoU损失可以使回归非常快，而GIoU损失几乎退化为IoU损失。\nDIoU还可以替换普通的IoU评价策略，应用于NMS中，使得NMS得到的结果更加合理和有效。\n实现代码：[3]\n \n \n 四、CIoU(Complete-IoU)\n论文考虑到bbox回归三要素中的长宽比还没被考虑到计算中，因此，进一步在DIoU的基础上提出了CIoU。其惩罚项如下面公式：\n \n 实现代码：[5] \n \n \n \n\n\nL(IoU)&#x3D;1-IoU,L(GIoU)&#x3D;1-GIoU\n\npenalty item 惩罚项\n\n\n\n用1替换\n\n\nYOLO训练技巧\n1.\n\n2.\n余弦退火学习率调整的原理是根据余弦函数的形状动态地调整学习率。它通过将学习率从一个较大的初始值逐渐减小到一个较小的最小值来控制训练过程中的学习率变化。\n具体实现步骤如下：\n\n设置一个最大学习率和最小学习率的范围。\n定义一个周期数（通常是训练的总迭代次数）。\n对于每个训练迭代，计算当前周期数与总周期数之间的比例。\n使用余弦函数来动态计算学习率，公式如下：\n\ntxtlr = lr_min + 0.5 * (lr_max - lr_min) * (1 + cos(epoch / T_total * pi))其中，lr表示当前学习率，epoch表示当前周期数，T_total表示总周期数，lr_max表示最大学习率，lr_min表示最小学习率。\n\n将计算得到的学习率应用于优化器中进行权重更新。\n\n使用余弦退火学习率调整可以在训练初期使用较大的学习率来快速收敛，然后逐渐减小学习率以细化模型的优化过程。这种方法在训练中期能够跳出局部最优解并找到更好的全局最优解，有助于提高模型的泛化性能和训练效果。\n3.\n4.\n5.遗传算法\n6.AMP\n7.\n激活函数\n\n\n\n\n优点：激活区域可以更多样\n","slug":"yolov5","date":"2023-12-16T12:04:59.000Z","categories_index":"","tags_index":"yolo","author_index":"Zgh"},{"id":"2dd54cd4b432aa48fbfa03c6312b571a","title":"deeplearning","content":"在 Pandas 中，.apply() 是用于对 DataFrame 或 Series 中的元素应用指定函数的方法。\n对于 DataFrame，.apply() 可以在行或列方向上应用函数。语法如下：\ntxtDataFrame.apply(func, axis=0)\nfunc 是要应用的函数，可以是一个已定义的函数，也可以是一个匿名函数（如 lambda 函数）。\naxis 是指定应用函数的方向，默认为 0，表示按列应用函数；设置为 1 则表示按行应用函数。\n\n对于 Series，.apply() 仅能在元素级别上应用函数，无需指定应用方向。语法如下：\ntxtSeries.apply(func)\nfunc 是要应用的函数，可以是一个已定义的函数，也可以是一个匿名函数（如 lambda 函数）。\n\n在上述代码中，.apply(lambda x: (x - x.mean()) / (x.std())) 就是将匿名函数 lambda x: (x - x.mean()) / (x.std()) 应用到 DataFrame 或 Series 中的每个元素上。结果是对 DataFrame 或 Series 中的每个元素进行标准化计算，并返回处理后的结果\n\n​                                                                                                         右–按照时间线展开图\n\n查询（自主提示）和键（非自主提示）之间的交互形成了注意力汇聚；注意力汇聚有选择地聚合了值（感官输入）以生成最终的输出。torch.repeat_interleave() 函数是 PyTorch 中的一个张量操作函数，用于生成一个重复值的张量。它的详细解释如下：\ntxt\ntorch.repeat_interleave(input, repeats, dim=None)参数说明：\n\ninput：输入张量。\nrepeats：重复次数，可以是一个整数、一个一维张量或一个与 input 张量形状相匹配的张量。\ndim（可选）：指定重复操作的维度。\n\n函数功能：\n\ntorch.repeat_interleave() 函数将输入张量 input 按指定的重复次数 repeats 进行重复，并生成一个新的张量。\n\ntxthttps://zhuanlan.zhihu.com/p/659067322\n\nsk-QxaDRZHsiyhtTFpB3JXHMkQ5fiK0AnEQZgM27mbiEIaIkF0G","slug":"deeplearning","date":"2023-12-16T02:57:26.000Z","categories_index":"","tags_index":"","author_index":"Zgh"},{"id":"61251d8b73997ae6479adce2ddc93d34","title":"Pytorch","content":"在 Python 中，lambda 是用来创建匿名函数的关键字。所谓匿名函数，即没有显式定义函数名的函数，通常用于需要临时定义简单函数的场景。\nlambda 函数的语法如下：\ntxtlambda arguments: expression其中：\n\narguments 是函数的参数，可以有多个参数，用逗号隔开。\nexpression 是函数的返回值计算表达式。\n\nlambda 函数通常用于需要一个函数，但是又不想正式定义一个函数的场景，比如作为其他函数的参数传递进去，或者在一些函数式编程的场景中使用。\nparams &#x3D; [W_xh, W_hh, b_h, W_hq, b_q]for param in params:        param.requires_grad_(True)\n目的是告诉 PyTorch 在模型训练过程中需要计算这些参数的梯度，并且在反向传播时对其进行更新。\nXt*Wxh + Ht−1*Whh=cat(Xt,Ht−1)*cat(Wxh,Whh)\ntorch.matmul(X, W_xh) + torch.matmul(H, W_hh)\ntorch.matmul(torch.cat((X, H), 1), torch.cat((W_xh, W_hh), 0))\n当一个类实现了 __call__ 方法时，它的实例对象可以像函数一样进行调用。这意味着你可以使用实例对象作为函数来调用，就好像调用一个函数一样。\n例如，假设有一个类 MyClass，并且实现了 __call__ 方法：\ntxtpython\nclass MyClass:\n    def __call__(self, x):\n        print(&quot;Calling MyClass with argument:&quot;, x)现在，你可以创建一个 MyClass 的实例，并将其作为函数进行调用：\ntxtpython\nobj = MyClass()\nobj(10)输出结果将是：\ntxt\nCalling MyClass with argument: 10所以class RNNModelScratch: #@save&quot;&quot;&quot;从零开始实现的循环神经网络模型&quot;&quot;&quot;def __init__(self, vocab_size, num_hiddens, device,get_params, init_state, forward_fn):self.vocab_size, self.num_hiddens = vocab_size, num_hiddensself.params = get_params(vocab_size, num_hiddens, device)self.init_state, self.forward_fn = init_state, forward_fndef __call__(self, X, state):X = F.one_hot(X.T, self.vocab_size).type(torch.float32)return self.forward_fn(X, state, self.params)\n可以这样调用：\nnet = RNNModelScratch(len(vocab), num_hiddens, d2l.try_gpu(), get_params,init_rnn_state, rnn)\nY, new_state = net(X.to(d2l.try_gpu()), state)\n**kwargs 是 Python 中的一种特殊语法，用于接收任意数量的关键字参数（keyword arguments）。在函数或方法的定义中，**kwargs 会将传递给函数的未命名关键字参数收集到一个字典中，其中字典的键是参数名，值是参数值。\n在这段代码中，**kwargs 被用作 NWKernelRegression 类的初始化方法 __init__ 的参数。通过使用 **kwargs，可以接收任意数量的关键字参数，并将它们存储为类的属性。这样做可以使代码更加灵活，允许用户在创建 NWKernelRegression 实例时传递额外的参数。\n例如，如果你创建了一个 NWKernelRegression 实例时传递了额外的参数，比如 model = NWKernelRegression(param1=10, param2=&#39;abc&#39;)，那么这些额外的参数会被收集到 kwargs 字典中，字典的键是参数名，值是参数值。你可以根据需要在 __init__ 方法中使用这些参数。\n总而言之，**kwargs 允许在函数或方法定义中接收任意数量的关键字参数，并将它们保存为字典以供后续使用。\n","slug":"Pytorch","date":"2023-12-15T14:50:19.000Z","categories_index":"","tags_index":"","author_index":"Zgh"},{"id":"b9663f58f18133b35bfe243f3e916a80","title":"Hello World","content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.\nQuick StartCreate a new postbash$ hexo new &quot;My New Post&quot;More info: Writing\nRun serverbash$ hexo serverMore info: Server\nGenerate static filesbash$ hexo generateMore info: Generating\nDeploy to remote sitesbash$ hexo deployMore info: Deployment\n","slug":"hello-world","date":"2023-12-15T14:27:11.801Z","categories_index":"","tags_index":"","author_index":"Zgh"}]